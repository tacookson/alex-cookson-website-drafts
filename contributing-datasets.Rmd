---
title: "Benefits of Curating Datasets"
author: "Alex Cookson"
date: "2020-08-19"
output: html_document
---

### Abstract

There are a few classic datasets, like mtcars or nycflights. They're okay, but they leave something to be desired for folks learning R: they're kind of boring.

There's a big difference between Okay Datasets and Great Datasets. Great Datasets prompt you to exclaim, "That's so cool!" They get your blood pumping and mind racing with questions you want answered. They give tremendous motivation to answer those questions. And in answering those questions, you'll probably learn some R.

I want you to curate Great Datasets. You'll contribute to the richness of our community, you'll learn some R yourself, and you'll feel fantastic when someone finds your Great Dataset and exclaims, "That's so cool!"



I've learned about web scraping, data cleaning, and even translating Japanese Kanji characters to English in my question to curate Great Datasets

My goal is to get you to go out and curate datasets: gather untidy data that already exists out in the world and get it into to a tidy, easily accessible format. Why?

- Curating datasets is a learning opportunity by itself (web scraping, data manipulation, APIs)
- It's a great way to contribute to the community while adding a little bit of your personality to the community at large
- The datasets we use are a reflection of our values as a community (iris's association with eugenics and Palmer Penguins as a proposed alternative)





### Who is the audience?

Anyone who pursues personal projects with R.


### What should readers take away from this post?

- "I'm going to put together some datasets!"
- This is a sequel of sorts to Ryan Timpe's [*Learning R with humorous side projects*](https://www.ryantimpe.com/post/rstudio/)


### Why should they think that?

1. Datasets we use are important (to us as learners). Having a "Wow! That's so cool!" dataset provides inherent motivation to learn because the dataset will prompt questions that you will feel *compelled* to answer, even if it means working through a new technique.
1. Datasets -- even just curating them -- let you express your individuality. They are a data scientist's fashion/style. Example: I've never met Ryan Timpe (https://www.ryantimpe.com/), but I'll bet he likes Golden Girls and dinosaurs.
1. Datasets we use are Important (to the community). Example: Iris dataset's association with Ronald Fisher and eugenics, and the proposed and enthusiastically received alternative dataset: Palmer Penguins.





### Original Outline

Who is the audience? Teachers (who put together instructional materials) and Learners (who are looking for projects to develop their skills)


Convince audience of two things:

1. You should care about datasets: The datasets we use to teach and learn are **important**. They are more than a means of illustrating or teaching a concept.
1. You should curate your own datasets: Datasets are an accessible way for learners to get their hands dirty with projects and they are a way to contribute to the community.


### You should care about datasets

Datasets as a way to engage learners:

- A compelling dataset will make someone exclaim, "Wow! That's so cool!"
    - Katherine Goode using bat movement data to illustrate `gganimate` on a Halloween-day presentation (https://www.datarepository.movebank.org/handle/10255/move.421) and (https://goodekat.github.io/presentations/2019-isugg-gganimate-spooky/slides.html#1)
    - Marine biology lecturers using cetaceans dataset from Tidy Tuesday (https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-12-18)

- Learners will feel compelled to ask questions and answer them, which flips the standard "data-technique" relationship. Instead of using data as a way to illustrate a technique, the technique becomes a means to getting an answer to a burning question out of the data.



Datasets reflect the values of our community:

- Iris dataset, one of the classic illustrative datasets, was first published by Ronald Fisher in the *Annals of Eugenics*
    - The first sentence of the foreward of the first issue of the *Annals of Eugenics* is: "The time seems fully ripe  for the issue of  a journal which shall devote its pages wholly to the scientific treatment of  racial problems in man." (https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1469-1809.1925.tb02036.x)
    - The first paragraph of the paper in which Iris appears states that Fisher is applying a principle from craniometry to Iris taxonomy (https://digital.library.adelaide.edu.au/dspace/bitstream/2440/15227/1/138.pdf)
- Blog post on the problems of Iris: https://armchairecology.blog/iris-dataset/

- Palmer Penguins dataset (https://github.com/allisonhorst/palmerpenguins) is a great alternative, without problematic connections to eugenics, with the bonus of being crazy-cute (https://github.com/allisonhorst/palmerpenguins/blob/master/man/figures/lter_penguins.png)
- Making data available is a step to making issues visible, because data can answer questions or provide new perspectives:
    - Public policy (e.g., biking count data)
    - Social issues (e.g, American Slavery and Juneteenth (https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-06-16/readme.md))
    - Highlighting things that don't get the attention they deserve (e.g, women's sports (https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-07-09))


### Learn from Curating!

- Goal: get already-existing data into a tidy format (ready for Tidy Tuesday!)
- Classic advice for learners is to work on a project. This is much easier if a dataset already taps into an existing interest.
- It's a repeatable project with lots of variation and degrees of difficulty, depending on the dataset being curated
- Things I've learned by curating datasets:
    - Webscraping with `rvest`
    - Extracting data from PDFs with `pdftools` (https://themockup.blog/posts/2020-04-03-beer-and-pdftools-a-vignette/)
    - Setting up and using Google Translate API with `googleLanguageR`
    - Iterating with `purrr`
    - Endless data cleaning of data "in the wild" with `dplyr` (joins, de-duplication, `pivot_wider`/`pivot_longer`, `coalesce`, `fill`, `case_when`, lots of regular expressions)
    
    
### Appendix

Main points:

- A compelling or interesting dataset can be a significant motivator for people to use and learn R
- The datasets we use are important (see iris dataset as a default vs. Palmer Penguins as a proposed replacement)
- Curating datasets is a learning opportunity in and of itself (especially data cleaning, web scraping, and creative problem-solving)
- Output of curating datasets -- the datasets themselves -- are a worthy and useful contribution to the R community as a whole (and beyond, since a CSV or TSV file is language-agnostic)
