---
title: "Vena Reviews"
author: "Alex Cookson"
date: "12/08/2020"
output: html_document
---

```{r setup}
library(tidyverse) # Data manipulation
library(rvest) # Scraping
library(tidytext) # Text analysis
library(textstem) # Lemmatization
library(glmnet) # LASSO model

theme_set(theme_minimal())
```

```{r scrape-function}
scrape_pros_cons <- function(url) {
  # Read website
  webpage <- read_html(url)
  
  # Plus/Minus
  icons <- webpage %>%
    html_nodes(".ugc .sprite") %>%
    html_attrs() %>%
    unlist() %>%
    as_tibble_col(column_name = "pro_con") %>%
    mutate(pro_con = str_remove(pro_con, "sprite sprite-proCon"))
  
  # Text
  text <- webpage %>%
    html_nodes(".ugc div:nth-child(2)") %>%
    html_text() %>%
    as_tibble_col(column_name = "text")
  
  # Merge
  return(bind_cols(icons, text))
}

```

```{r scrape-pros-cons}
pros_cons_raw <- tibble(base_url = "https://www.trustradius.com/products/vena-solutions/reviews?f=",
       review_num = seq(0, 125, by = 25)) %>%
  mutate(scrape_url = paste0(base_url, review_num)) %>%
  select(scrape_url) %>%
  mutate(data = map(scrape_url, possibly(scrape_pros_cons, NULL))) %>%
  unnest(data) %>%
  select(-scrape_url)
```

```{r clean-and-tokenize-text}
pros_cons <- pros_cons_raw %>%
  mutate(bullet_id = row_number(),
         score = ifelse(pro_con == "Plus", 1, 0)) %>%
  select(bullet_id, text, score)

tokens <- pros_cons %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words, by = "word") %>%
  filter(str_detect(word, "[A-Za-z]")) %>%
  mutate(word = lemmatize_words(word)) %>%
  distinct(bullet_id, word, .keep_all = TRUE)
```

```{r lasso-model-prep}
tokens_clean <- tokens %>%
  mutate(token_id = row_number()) %>%
  select(token_id, word, score)

token_matrix <- tokens_clean %>%
  select(token_id, word) %>%
  cast_sparse(token_id, word)

ids <- as.integer(rownames(token_matrix))

ratings <- tokens_clean$score[ids]
```

```{r lasso-model}
cv_lasso <- cv.glmnet(token_matrix,
                      ratings,
                      nfolds = 100)

cv_lasso_tidy <- cv_lasso$glmnet.fit %>%
  tidy() %>%
  filter(lambda == cv_lasso$lambda.1se)
```

```{r sentiment-lexicon}
cv_lasso_tidy %>%
  mutate(direction = ifelse(estimate > 0, "Positive", "Negative")) %>%
  filter(term != "(Intercept)") %>%
  group_by(direction) %>%
  top_n(12, wt = abs(estimate)) %>%
  ungroup() %>%
  mutate(term = fct_reorder(term, estimate)) %>%
  ggplot(aes(estimate, term, fill = direction)) +
  geom_col()
```


Bigrams

```{r}
bigrams <- pros_cons %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, into = c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !is.na(word1),
         !is.na(word2)) %>%
  unite("bigram", word1, word2, sep = " ") %>%
  distinct(bullet_id, bigram, .keep_all = TRUE)
```

```{r lasso-model-prep}
bigrams_clean <- bigrams %>%
  mutate(bigram_id = row_number()) %>%
  select(bigram_id, bigram, score)

bigram_matrix <- bigrams_clean %>%
  select(bigram_id, bigram) %>%
  cast_sparse(bigram_id, bigram)

ids <- as.integer(rownames(bigram_matrix))

ratings <- bigrams_clean$score[ids]
```

```{r lasso-model}
cv_lasso <- cv.glmnet(bigram_matrix,
                      ratings,
                      nfolds = 100)

cv_lasso_tidy <- cv_lasso$glmnet.fit %>%
  tidy() %>%
  filter(lambda == cv_lasso$lambda.min)
```

```{r sentiment-lexicon}
cv_lasso_tidy %>%
  mutate(direction = ifelse(estimate > 0, "Positive", "Negative")) %>%
  filter(term != "(Intercept)") %>%
  group_by(direction) %>%
  top_n(12, wt = abs(estimate)) %>%
  ungroup() %>%
  mutate(term = fct_reorder(term, estimate)) %>%
  ggplot(aes(estimate, term, fill = direction)) +
  geom_col()
```


